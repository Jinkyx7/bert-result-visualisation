{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation for Excel visualisation\n",
    "\n",
    "This notebook combines sentence-level predictions with page counts, aggregates by company/year/label, and computes metrics across multiple probability thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load combined predictions\n",
    "\n",
    "`combine_csvs.py` output should be in `outputs/final_result.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = Path(\"outputs/final_result.csv\")\n",
    "df = pd.read_csv(preds_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape to long format (label + probability) and compute word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"prob_fin_label\": \"fin\",\n",
    "    \"prob_soc_label\": \"soc\",\n",
    "    \"prob_env_label\": \"env\",\n",
    "    \"prob_maori_label\": \"maori\",\n",
    "}\n",
    "\n",
    "df[\"word_count\"] = df[\"sentence\"].astype(str).str.split().str.len()\n",
    "\n",
    "long_df = df.melt(\n",
    "    id_vars=[\"company\", \"year\", \"word_count\"],\n",
    "    value_vars=list(label_map.keys()),\n",
    "    var_name=\"label\",\n",
    "    value_name=\"probability\",\n",
    ")\n",
    "long_df[\"label\"] = long_df[\"label\"].map(label_map)\n",
    "long_df = long_df.dropna(subset=[\"label\", \"probability\"])\n",
    "long_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic checks and required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = {\"company\", \"year\", \"sentence\", \"prob_fin_label\", \"prob_soc_label\", \"prob_env_label\", \"prob_maori_label\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df = df.dropna(subset=[\"company\", \"year\", \"sentence\"])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and aggregate page counts\n",
    "\n",
    "`merge_page_counts.py` output should be in `outputs/page_counts_merged.csv`. Pages are summed per company/year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path = Path(\"outputs/page_counts_merged.csv\")\n",
    "pages = pd.read_csv(pages_path)\n",
    "\n",
    "required_pages = {\"company\", \"year\", \"pages\"}\n",
    "missing_pages = required_pages - set(pages.columns)\n",
    "if missing_pages:\n",
    "    raise ValueError(f\"Missing page count columns: {missing_pages}\")\n",
    "\n",
    "pages[\"year\"] = pd.to_numeric(pages[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "pages[\"pages\"] = pd.to_numeric(pages[\"pages\"], errors=\"coerce\")\n",
    "page_counts = (\n",
    "    pages.dropna(subset=[\"company\", \"year\", \"pages\"])\n",
    "    .groupby([\"company\", \"year\"], as_index=False)[\"pages\"]\n",
    "    .sum()\n",
    ")\n",
    "page_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_YEAR = int(df[\"year\"].min())\n",
    "\n",
    "def summarize_over_threshold(df_long: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    over_mask = df_long[\"probability\"] > threshold\n",
    "    enriched = df_long.assign(\n",
    "        over=over_mask,\n",
    "        over_word_count=lambda d: d[\"word_count\"].where(over_mask, 0),\n",
    "    )\n",
    "\n",
    "    agg = (\n",
    "        enriched\n",
    "        .groupby([\"company\", \"year\", \"label\"])\n",
    "        .agg(\n",
    "            total_sentences=(\"probability\", \"size\"),\n",
    "            total_words=(\"word_count\", \"sum\"),\n",
    "            over_count=(\"over\", \"sum\"),\n",
    "            over_share=(\"over\", \"mean\"),\n",
    "            over_word_count=(\"over_word_count\", \"sum\"),\n",
    "            mean_prob_over=(\"probability\", lambda s: s[s > threshold].mean() if (s > threshold).any() else 0.0),\n",
    "            sum_prob_over=(\"probability\", lambda s: s[s > threshold].sum()),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    agg = agg.merge(page_counts, on=[\"company\", \"year\"], how=\"left\")\n",
    "    agg[\"pages\"] = agg[\"pages\"].replace({0: pd.NA})\n",
    "\n",
    "    agg[\"over_sentence_word_share\"] = agg[\"over_count\"] / agg[\"total_words\"]\n",
    "    agg[\"over_word_share\"] = agg[\"over_word_count\"] / agg[\"total_words\"]\n",
    "    agg[\"sentences_per_page\"] = agg[\"total_sentences\"] / agg[\"pages\"]\n",
    "    agg[\"over_sentences_per_page\"] = agg[\"over_count\"] / agg[\"pages\"]\n",
    "    agg[\"over_words_per_page\"] = agg[\"over_word_count\"] / agg[\"pages\"]\n",
    "\n",
    "    agg[\"theme_rate_per_1000_words\"] = np.where(\n",
    "        agg[\"total_words\"] > 0,\n",
    "        (agg[\"over_count\"] / agg[\"total_words\"]) * 1000,\n",
    "        0.0,\n",
    "    )\n",
    "    non_theme = agg[\"total_sentences\"] - agg[\"over_count\"]\n",
    "    agg[\"theme_ratio_per_1000_sentences\"] = np.where(\n",
    "        non_theme > 0,\n",
    "        (agg[\"over_count\"] / non_theme) * 1000,\n",
    "        0.0,\n",
    "    )\n",
    "\n",
    "    base_counts = (\n",
    "        agg.loc[agg[\"year\"] == BASE_YEAR, [\"label\", \"company\", \"over_count\"]]\n",
    "        .drop_duplicates(subset=[\"label\", \"company\"])\n",
    "        .set_index([\"label\", \"company\"])[\"over_count\"]\n",
    "    )\n",
    "    agg[\"theme_index_base\"] = agg.apply(\n",
    "        lambda row: (row[\"over_count\"] / base_counts.get((row[\"label\"], row[\"company\"]), 0)) * 100\n",
    "        if base_counts.get((row[\"label\"], row[\"company\"]), 0) > 0 else 0.0,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    agg[\"avg_theme_score_relevant\"] = agg[\"mean_prob_over\"]\n",
    "    agg[\"threshold\"] = threshold\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build summaries for all thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "summaries = [summarize_over_threshold(long_df, thr) for thr in thresholds]\n",
    "summary_all = pd.concat(summaries, ignore_index=True)\n",
    "summary_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save for Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"outputs/aggregation_thresholds.csv\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary_all.to_csv(output_path, index=False)\n",
    "print(f\"Saved: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}